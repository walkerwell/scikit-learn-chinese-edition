## 1. 广义线性模型


在下面几种回归问题的表示方法中，我们的目标值(输出)是基于输入值的线性组合后的结果。
用数学符号描述：
\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p
在模型中，我们通过coef_参数来指定 w = (w_1,..., w_p) 、通过intercept_参数来指定w_0。
想看广义线性模型在分类问题上面的表现，请看- [逻辑回归](逻辑回归.md)


#### 1.1 一般的最小二乘


线性回归是去拟合一个由系数w = (w_1, ..., w_p)构成的线性模型：最小化数据集中所有的真实值与模型预测值的差的平方和。并且对预测值做线性近似。
数学公式表示为：
\underset{w}{min\,} {|| X w - y||_2}^2

线性回归调用fit方法的中传递X、Y，并将系数W的结果保存在参数coef_中：
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])

然而,普通最小二乘估计系数依赖于独立模型的条件。当条件间不是互相独立的并且和特征间有近似的线性依赖，设计矩阵接近奇异结果。最小二乘对于预测值的随机误差会高度的敏感，产生较大的方差。这种情况可能出现多重共线性，举个例子，当数据收集时没有实验设计。

示例：
- [线性回归示例](线性回归示例.md)

#### 1.2 岭回归
